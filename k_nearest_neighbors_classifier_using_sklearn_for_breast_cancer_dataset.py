# -*- coding: utf-8 -*-
"""K-Nearest Neighbors Classifier using sklearn for Breast Cancer Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uQW_fscefA7IPA4VlxDJKrlgri015J4r

# Step 1: Importing the required Libraries
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns

"""# Step 2: Reading the Dataset"""

import pandas as pd
from sklearn.datasets import load_iris

# Load iris dataset from sklearn
iris = load_iris()

# Create DataFrame
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target  # 0, 1, 2 as class labels

print(df.head())

# Save to CSV
df.to_csv("iris.csv", index=False)
print("\nSaved iris.csv successfully!")

"""# STEP 1 – Data Preparation"""

import pandas as pd

# Load CSV file
df = pd.read_csv("iris.csv")

print("First 5 rows:")
print(df.head())

print("\nShape of dataset:", df.shape)

"""## STEP 2 – Basic EDA (Exploratory Data Analysis)"""

import pandas as pd

# Already loaded as df
print("\nInfo:")
print(df.info())

print("\nSummary statistics:")
print(df.describe())

print("\nClass distribution (species):")
print(df['species'].value_counts())

# Check missing values
print("\nMissing values per column:")
print(df.isnull().sum())

import matplotlib.pyplot as plt

# Histogram of each feature
df.hist(figsize=(8, 6))
plt.tight_layout()
plt.show()

# Pairplot by class (needs seaborn)
import seaborn as sns
sns.pairplot(df, hue='species')
plt.show()

# Correlation heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(df.corr(), annot=True, fmt=".2f")
plt.show()

"""# STEP 3 – Preprocessing

For KNN:

Separate features (X) and target (y)

Train-test split

Feature scaling (very important for distance-based models like KNN)
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Split features & target
X = df.drop('species', axis=1).values   # all columns except target
y = df['species'].values               # target

# 2. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,        # 20% test
    random_state=42,
    stratify=y            # maintain class distribution
)

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

# 3. Feature scaling (fit on train, transform on both)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# STEP 4 – Model Building Using KNN Classifier

We will:

Try different values of k

Pick the one with best accuracy on validation (here we just use test as simple demo)
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

best_k = None
best_acc = 0

for k in range(1, 16):   # Try k from 1 to 15
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    y_pred_k = knn.predict(X_test_scaled)
    acc_k = accuracy_score(y_test, y_pred_k)
    print(f"k={k}, Accuracy={acc_k:.4f}")

    if acc_k > best_acc:
        best_acc = acc_k
        best_k = k

print(f"\nBest k: {best_k} with accuracy: {best_acc:.4f}")

knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train_scaled, y_train)

"""# STEP 5 – Model Evaluation

Use:

Accuracy

Confusion matrix

Classification report
"""

from sklearn.metrics import classification_report, confusion_matrix

# Predictions
y_pred = knn.predict(X_test_scaled)

# Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Final KNN Accuracy on Test Set: {acc:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - KNN")
plt.show()

"""# STEP 6 – Model Deployment (Saving the Model)

### For deployment, it’s better to combine scaling + model into a single Pipeline, then save it.
"""

from sklearn.pipeline import Pipeline
import joblib

# Build a pipeline: StandardScaler -> KNN
model_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=best_k))
])

# Train pipeline on full training data (unscaled X_train; pipeline will handle scaling)
model_pipeline.fit(X_train, y_train)

# Save the pipeline as a .pkl file
joblib.dump(model_pipeline, "knn_iris_model.pkl")
print("Model saved as knn_iris_model.pkl")

"""# STEP 7 – Using the Saved Model for Prediction (Inference Script)"""

import numpy as np
import joblib

# Load the saved pipeline
model = joblib.load("knn_iris_model.pkl")

# Example new data: [sepal length, sepal width, petal length, petal width]
new_sample = np.array([[5.1, 3.5, 1.4, 0.2]])

# Predict
pred_class = model.predict(new_sample)[0]
print("Predicted class (numeric label):", pred_class)

# If you want names instead of numbers:
class_names = ["setosa", "versicolor", "virginica"]
print("Predicted species name:", class_names[pred_class])